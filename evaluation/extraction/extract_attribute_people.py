import os
import glob
import random
import json
import openai
import time
import json
import numpy as np
import sys
sys.path.append("../")
from gpt_model import llm, set_key
import tqdm
import argparse

def main(args):
    set_key()
    
    path = args.caption_path

    with open(path, "r") as f:
        generated_caps = json.load(f)
        
    messages = [
        {
            "role": "system", 
            "content": "You are a language assistant that helps to extract the ranking from given sentences." 
        },
    ]
    content = """
        Given an image with a caption that is generated by a vision language model. 
        Please act as a linguistic master and extract the total number and colors of all objects as mentioned in the captions.
        Your answer should be a dict of this format: {"total_num_of_people": "(NUM, PERSON)", "clothes": {"ORDER": "person": "PERSON", "object": "(ATTRIBUTE, OBJECT)", "action": "ACTION"}}. OBJECT can be clothes or accessories (e.g., bags, socks).
        
        For clarity, consider these examples:
        
        ### Example 1:
        - Caption: This is an image of three people walking on a beach with a foggy backdrop.\n\nFrom left to right:\n\n1. The first person is wearing a dark green jacket with a hood over their head, blue jeans, and black shoes.\n\n2. The second person has on a bright blue jacket with a hood, burgundy shorts over black leggings, and brown shoes with white soles.\n\n3. The third individual is wearing a black jacket with a hood, blue jeans, and dark shoes with a black backpack.\n\nAll of them are facing away from the camera, walking towards the horizon of the beach where grassy areas can be seen, and there are patches of water, wet sand, and scattered seaweed on the beach.
        - Result: {"total_num_of_objects": "(3, people)", "clothes": {"1": {"person": "person", "object": "(green, jacket), (blue, jeans), (black, shoes)", "action": "walking"}, "2": {"person": "person", "object": "(blue, jacket), (black, leggings), (brown, shoes)", "action": "walking"}, "3": {"person": "person", "object": "(black, jacket), (blue, jeans), (dark, shoes), (black, backpace)", "action": "walking"}}}

        With these examples in mind, please help me extract the relations based on the information in the caption. You should only return a JSON file without any explanations.
        Here is the caption:
        [CAPTION]
    """
    
    outpath = args.output_file_path
    
    for image_id, cap in tqdm.tqdm(list(generated_caps.items())):
        cap_to_gpt = cap["generated_caption"]

        input = content.replace("[CAPTION]", cap_to_gpt) 
        prompt = messages + [{"role": "user", "content": input}]
        llm_output = llm(prompt)
        current_output = {
            "image_id": image_id,
            "response": llm_output,
            "generated_caption": cap_to_gpt
        }
        with open(outpath, "a") as f:
            f.write(json.dumps(current_output)+"\n")

        print(llm_output)
        
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="objects extracting")
    parser.add_argument("-ip", "--caption_path", type=str, required=True)
    parser.add_argument("-op", "--output_file_path", type=str, required=True)
    args = parser.parse_args()
    main(args)

