import os
import glob
import random
import json
import openai
import time
import json
import numpy as np
import sys
sys.path.append("../")
from gpt_model import llm, set_key
import tqdm
import argparse

def main(args):
    set_key()
    
    path = args.caption_path 
    
    with open(path, 'r') as f:
        generated_caps = json.load(f)
        
    messages = [
        {
            "role": "system", 
            "content": "You are a language assistant that helps to extract information from given sentences." 
        },
    ]
    content = """
        Given an image with a caption that is generated by a vision language model. 
        Please act as a linguistic master and extract a set of words describing the spatial or positional relations between all the visual objects from the captions.
        Your answer should be a list of values that are in format of object1 relation with object2 with the relation being left, right, top, bottom, middle etc.
        Do not extract the attribute along with the object and don't extract any relation that is an verb, replace it with simply which object is (on or to the left or etc) the other object or the image. Formulate your response into a JSON object with the key being "relations" and the value being a list of relations. 
        If there are no relations found, please return an empty list.
        For clarity, consider these examples:
        
        ### Example 1:
        - Caption: The relationship between the objects in the image is that the knife is on the right side of the cutting board, the banana is on the left side of the cutting board, and the cheese is underneath the cutting board.
        - relations: [knife is on the right side of the cutting board, the banana is on the left side of the cutting board, the cheese is underneath the cutting board]
        
        ### Example 2:
        - Caption: The relationship between the objects in the image is that a man is playing with his dog on a sandy beach. The man is standing on the left side of the image, while the dog is running on the right side of the image. The dog is chasing a frisbee thrown by the man, who is trying to catch the frisbee before the dog does
        - relations: ["the man is on the beach, the dog is on the beach, the man is on the left side of the image, the man's dog is on the right side of the image]
        
        ### Example 3:
        - Caption: From the view of the observer, the objects in the image can be described as follows: 
        1. On the left side of the image, there is a wooden chair with a red cushion. 
        The chair is placed near a window, which allows natural light to enter the room. 
        2. On the right side of the image, there is a small table with a vase of flowers on it. 
        The vase contains pink and purple tulips, adding a touch of color and beauty to the room. 
        3. At the bottom of the image, there is a rug with a floral pattern. 
        The rug complements the vase of flowers on the table and enhances the overall aesthetic of the room.
        - relations: [wooden chair is on the left side of the image,  chair is near a window, a vase of flowers on the table, a small table is on the right side of the image, a vase of flower is on the right side of the image, a rug at the bottom of the image]
        Notice that here in the sentence "On the right side of the image, there is a small table with a vase of flowers on it." We can extract three relations from it: 1. a vase of flowers on the table, 2. a small table is on the right side of the image, 3. a vase of flower is on the right side of the image.
        
        With these examples in mind, please help me extract the relations based on the information in the caption.
        Here is the caption:
        {}
    """
    
    outpath = args.output_file_path
    for image_id, cap in tqdm.tqdm(list(generated_caps.items())):
        cap_to_gpt = cap['generated_caption']

        input = content.format(cap_to_gpt)      
        prompt = messages + [{"role": "user", "content": input}]
        llm_output = llm(prompt)
        print("prompt",prompt)
        #print(llm_output)
        current_output = {
            "image_id": image_id,
            "response": llm_output,
            "generated_caption": cap_to_gpt
        }
        with open(outpath, "a") as f:
            f.write(json.dumps(current_output)+"\n")

        print(llm_output)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="objects extracting")
    parser.add_argument("-ip", "--caption_path", type=str, required=True)
    parser.add_argument("-op", "--output_file_path", type=str, required=True)
    args = parser.parse_args()
    main(args)

